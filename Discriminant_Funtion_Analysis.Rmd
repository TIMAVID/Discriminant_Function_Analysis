---
title: "Discriminant_Function_Analysis"
author: "David Ledesma and Hans Bilger"
date: "4/25/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# What is a discriminant function anaysis (DFA) used for?

### A DFA can be used to predict group membership based on discriminating continous variable and how well those groups can be differentiated by those variables.

### In essence, a DFA is the reverse of a multivariate analysis of variance or MANOVA. In a MANOVA, the groups are the independent variables and the predictors are the dependent variables. For a DFA, the groups are the dependent variables while the predictors are the independent variables. 

# Assumptions and limitations

### 1) Unequal sample sizes are not a problem; however, the sample size within groups must be greater than the number of independent variable predictors (Rule of thumb: ~ 4 to 5 as many observations as predictor variables).

### 2) Outliers can cause problems when assesing significance and should transformed or removed. Normally distributed variables are prefered, but variables not normally distributed will not influence the significance tests as long as the non-normality is not due to outliers.

### 3) It is important that the independent variables must not be correlated with one another and have low multicollinearity. 

### 4) It is important to evaluate the degree to which the variance and co-variance matricies are herterogeneic within groups. 

### Discriminant function anaysis is similar to logistic regression and in fact both can be used to answer the same research questions. In many cases logistic regression may be preferable to a discriminant function anaysis because it has less assumptions and restrictions. However, a discriminant function anaysis may be prefered when looking at many different classifications. 

# How does a discriminant function anaysis work?

### The analysis creates linear combinations of predictors, so called discriminant functions. The number of functions is equal to the number of groups or the number of predictors, whichever is smaller. Each discriminant function maximizes the amount of discrimination between groups while also remaining non-correlated with the other functions.

```{r}
library(MASS) # required for the lda() function
library(caret) 
library(tidyverse)

# Load the data
data("iris")
# Split the data into training (80%) and test set (20%)
set.seed(123)
training.samples <- iris$Species %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data <- iris[training.samples, ]
test.data <- iris[-training.samples, ]

# Loading in data from separate files

learning_dat <- read.csv("YourLearningData.csv") # read in the data you will use to teach the analysis what each category looks like - minimally, you need a column for category (in your case, individual) plus one column for each of your variables

experimental_dat <- read.csv("YourExperimentalData.csv") # read in the data for which you will be predicting categories - this should be formatted exactly like your learing data

LDA_object <- lda(Category ~ Variable_1 + Variable_2 + Variable_3 + Variable_etc, data = learning_dat) # creates an object that is the results of the learning phase of the LDA - this is what you will use to predict what category each subsequent set of variables you feed it most likely belong to

Predict_results <- predict(LDA_object, newdata = experimental_dat) # use the LDA results object to predict the category for your experimental data - NOTE: if you have multiple rows in your experimental data, you'll need to use a for loop to compare each row to your LDA object - try this on your own first and if you can't make it work, I'll show you what I did

```
















